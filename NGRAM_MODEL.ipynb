{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theantigone/ngram-java-ai/blob/main/NGRAM_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oESE95McS5mN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "923df0a5-1692-4750-add8-f9f9fecf3116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KnHqXZm2LVZ"
      },
      "outputs": [],
      "source": [
        "from pygments.lexers.jvm import JavaLexer\n",
        "from pygments.token import Token\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "\n",
        "def tokenize_java_code(code):\n",
        "    \"\"\"Tokenizes Java code using Pygments.\"\"\"\n",
        "    lexer = JavaLexer()\n",
        "    return [t[1] for t in lexer.get_tokens(code) if t[0] not in Token.Text]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd6jqwVI7ssu"
      },
      "outputs": [],
      "source": [
        "def build_ngram_model(corpus, n):\n",
        "    \"\"\"Builds an N-gram model from tokenized Java methods.\"\"\"\n",
        "    ngram_counts = defaultdict(int)\n",
        "    context_counts = defaultdict(int)\n",
        "\n",
        "    for method in corpus:\n",
        "        tokens = tokenize_java_code(method)\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngram = tuple(tokens[i:i+n])\n",
        "            context = tuple(tokens[i:i+n-1])\n",
        "            ngram_counts[ngram] += 1\n",
        "            context_counts[context] += 1\n",
        "\n",
        "    print(f\"Number of N-grams: {len(ngram_counts)}\")\n",
        "    return ngram_counts, context_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEUJriH57v_F"
      },
      "outputs": [],
      "source": [
        "def compute_probabilities(ngram_counts, context_counts):\n",
        "    \"\"\"Computes conditional probabilities for N-grams.\"\"\"\n",
        "    probabilities = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        context = ngram[:-1]\n",
        "        probabilities[ngram] = count / context_counts[context]\n",
        "    return probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTFQ5cCG7xaO"
      },
      "outputs": [],
      "source": [
        "def perplexity(test_corpus, probabilities, n):\n",
        "    \"\"\"Computes perplexity for the N-gram model on a test set.\"\"\"\n",
        "    log_prob_sum = 0\n",
        "    N = 0\n",
        "\n",
        "    for method in test_corpus:\n",
        "        tokens = tokenize_java_code(method)\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngram = tuple(tokens[i:i+n])\n",
        "            prob = probabilities.get(ngram, 1e-6)  # Smoothing for unseen cases\n",
        "            log_prob_sum += math.log2(prob)\n",
        "            N += 1\n",
        "\n",
        "    return 2 ** (-log_prob_sum / N)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vos0hWE07y1Y"
      },
      "outputs": [],
      "source": [
        "def iterative_prediction_x(probabilities, context, n):\n",
        "    \"\"\"Predicts the next token iteratively given a starting context.\"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for _ in range(10):  # Limit predictions to avoid infinite loops\n",
        "        candidates = {ngram[-1]: prob for ngram, prob in probabilities.items() if ngram[:-1] == context}\n",
        "\n",
        "        if not candidates:\n",
        "            break  # Stop if no valid prediction\n",
        "\n",
        "        next_token = max(candidates, key=candidates.get)  # Most probable token\n",
        "        predictions.append((next_token, round(candidates[next_token], 3)))\n",
        "        context = context[1:] + (next_token,)  # Update context\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whZOvJmK70X1"
      },
      "outputs": [],
      "source": [
        "def generate_results_json(test_corpus, probabilities, n, filename):\n",
        "    \"\"\"Generates a JSON file with token predictions and probabilities.\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for i, method in enumerate(test_corpus[:100]):  # Limit to 100 examples\n",
        "        tokens = tokenize_java_code(method)[:n-1]\n",
        "        context = tuple(tokens)\n",
        "        results[str(i)] = iterative_prediction_x(probabilities, context, n)\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    print(f\"Saved predictions to {filename}\")\n",
        "    # Brief example\n",
        "    if results:\n",
        "        first_method_predictions = results[\"0\"]\n",
        "        if first_method_predictions:\n",
        "            print(f\"First prediction for method 0: {first_method_predictions[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5Bqt9Q_o9eX"
      },
      "outputs": [],
      "source": [
        "def generate_vocabulary(corpus):\n",
        "    \"\"\"Generates a vocabulary from the given corpus (training + eval + test).\"\"\"\n",
        "    vocab = set()\n",
        "    for method in corpus:\n",
        "        tokens = tokenize_java_code(method)\n",
        "        vocab.update(tokens)\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arqmm-_CtU7u"
      },
      "outputs": [],
      "source": [
        "def load_methods_from_file(filename):\n",
        "    \"\"\"\n",
        "    Reads a file containing methods enclosed in quotes and extracts each method as a single string.\n",
        "    Uses a regular expression with DOTALL to capture methods spanning multiple lines.\n",
        "    \"\"\"\n",
        "    with open(filename, \"r\") as f:\n",
        "        content = f.read()\n",
        "    # Extract text between double quotes (non-greedy) over multiple lines.\n",
        "    methods = re.findall(r'\"(.*?)\"', content, re.DOTALL)\n",
        "    # Strip whitespace from each method and filter out any empty strings.\n",
        "    methods = [method.strip() for method in methods if method.strip()]\n",
        "    return methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "NAjzCDKQ71Vh"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     # Load dataset from cleaned corpus (e.g., 7561 methods)\n",
        "#     with open(\"/content/drive/Shareddrives/CSCI_420/data/new.txt\", \"r\") as f:\n",
        "#         corpus = [line.strip() for line in f]\n",
        "\n",
        "#     print(f\"Total methods in corpus: {len(corpus)}\")  # Expected: 7561 methods\n",
        "\n",
        "#     # Vocabulary Generation (using training + eval + test sets)\n",
        "#     vocab = generate_vocabulary(corpus)\n",
        "#     print(vocab)\n",
        "#     print(f\"Vocabulary size: {len(vocab)}\")  # ZZZ number of code tokens\n",
        "\n",
        "#     # Shuffle and split dataset: 80% training, 10% eval, 10% test\n",
        "#     random.shuffle(corpus)\n",
        "#     train_set = corpus[:int(0.8 * len(corpus))]\n",
        "#     eval_set = corpus[int(0.8 * len(corpus)):int(0.9 * len(corpus))]\n",
        "#     test_set = corpus[int(0.9 * len(corpus)):]\n",
        "\n",
        "#     print(f\"Train set: {len(train_set)} methods\")\n",
        "#     print(f\"Eval set: {len(eval_set)} methods\")\n",
        "#     print(f\"Test set: {len(test_set)} methods\")\n",
        "\n",
        "#     # Model Training & Evaluation on Student's Data\n",
        "#     # Experiment with n = 3, n = 5, and n = 9; select best based on perplexity\n",
        "#     best_n = None\n",
        "#     best_perplexity = float(\"inf\")\n",
        "#     results = {}\n",
        "\n",
        "#     for n in [3, 5, 9]:\n",
        "#         print(f\"Evaluating {n}-gram model on training data\")\n",
        "#         ngram_counts, context_counts = build_ngram_model(train_set, n)\n",
        "#         probabilities = compute_probabilities(ngram_counts, context_counts)\n",
        "\n",
        "#         pp = perplexity(eval_set, probabilities, n)\n",
        "#         results[n] = pp\n",
        "#         print(f\"{n}-gram model Perplexity on Eval set: {pp}\")\n",
        "\n",
        "#         if pp < best_perplexity:\n",
        "#             best_perplexity = pp\n",
        "#             best_n = n\n",
        "\n",
        "#     print(f\"Selected best N: {best_n} (Perplexity: {best_perplexity})\")\n",
        "#     # According to our report, n = 3 was selected (with perplexity ~91.297)\n",
        "\n",
        "#     # Train best model on full training set\n",
        "#     best_ngram_counts, best_context_counts = build_ngram_model(train_set, best_n)\n",
        "#     best_probabilities = compute_probabilities(best_ngram_counts, best_context_counts)\n",
        "\n",
        "#     # Compute perplexity on the full test set for student model\n",
        "#     test_pp = perplexity(test_set, best_probabilities, best_n)\n",
        "#     print(f\"Test set Perplexity for {best_n}-gram model: {test_pp}\")  # Replace YYY.00000 with actual value\n",
        "\n",
        "#     # Generate JSON output for student model (first 100 predictions)\n",
        "#     generate_results_json(test_set, best_probabilities, best_n, \"results_student_model.json\")\n",
        "\n",
        "#     # Training, Evaluation, and Testing on the Instructor-Provided Corpus\n",
        "#     with open(\"/content/drive/Shareddrives/CSCI_420/data/training.txt\", \"r\") as f:\n",
        "#         instructor_corpus = [line.strip() for line in f]\n",
        "\n",
        "#     print(f\"Total methods in instructor's corpus: {len(instructor_corpus)}\")\n",
        "#     random.shuffle(instructor_corpus)\n",
        "#     instructor_train_set = instructor_corpus[:int(0.8 * len(instructor_corpus))]\n",
        "#     instructor_eval_set = instructor_corpus[int(0.8 * len(instructor_corpus)):int(0.9 * len(instructor_corpus))]\n",
        "\n",
        "#     best_instructor_n = None\n",
        "#     best_instructor_perplexity = float(\"inf\")\n",
        "\n",
        "#     for n in [3, 5, 9]:\n",
        "#         print(f\"Evaluating {n}-gram model on instructor's training data\")\n",
        "#         ngram_counts, context_counts = build_ngram_model(instructor_train_set, n)\n",
        "#         probabilities = compute_probabilities(ngram_counts, context_counts)\n",
        "\n",
        "#         pp = perplexity(instructor_eval_set, probabilities, n)\n",
        "#         print(f\"{n}-gram model Perplexity on Instructor's Eval set: {pp}\")\n",
        "#         if pp < best_instructor_perplexity:\n",
        "#             best_instructor_perplexity = pp\n",
        "#             best_instructor_n = n\n",
        "\n",
        "#     print(f\"Best N for instructor model: {best_instructor_n} (Perplexity: {best_instructor_perplexity})\")\n",
        "#     # According to our report, n = 3 was selected for the instructor corpus as well\n",
        "\n",
        "#     # Train best instructor model\n",
        "#     best_instructor_ngram_counts, best_instructor_context_counts = build_ngram_model(instructor_train_set, best_instructor_n)\n",
        "#     best_instructor_probabilities = compute_probabilities(best_instructor_ngram_counts, best_instructor_context_counts)\n",
        "\n",
        "#     # Compute perplexity on test set for instructor model\n",
        "#     instructor_test_pp = perplexity(test_set, best_instructor_probabilities, best_instructor_n)\n",
        "#     print(f\"Test set Perplexity for instructor's {best_instructor_n}-gram model: {instructor_test_pp}\")\n",
        "\n",
        "#     # Generate JSON output for instructor model (first 100 predictions)\n",
        "#     generate_results_json(test_set, best_instructor_probabilities, best_instructor_n, \"results_teacher_model.json\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvLKoMbC8nYn"
      },
      "outputs": [],
      "source": [
        "def student_train():\n",
        "    # Load dataset from cleaned corpus (e.g., 7561 methods)\n",
        "    with open(\"/content/drive/Shareddrives/CSCI_420/data/new.txt\", \"r\") as f:\n",
        "        corpus = [line.strip() for line in f]\n",
        "\n",
        "    print(f\"Total methods in corpus: {len(corpus)}\")  # Expected: 7561 methods\n",
        "\n",
        "    # Vocabulary Generation (using training + eval + test sets)\n",
        "    vocab = generate_vocabulary(corpus)\n",
        "    # print(vocab)\n",
        "    print(f\"Vocabulary size: {len(vocab)}\")  # ZZZ number of code tokens\n",
        "\n",
        "    # Shuffle and split dataset: 80% training, 10% eval, 10% test\n",
        "    random.shuffle(corpus)\n",
        "    train_set = corpus[:int(0.8 * len(corpus))]\n",
        "    eval_set = corpus[int(0.8 * len(corpus)):int(0.9 * len(corpus))]\n",
        "    test_set = corpus[int(0.9 * len(corpus)):]\n",
        "\n",
        "    print(f\"Train set: {len(train_set)} methods\")\n",
        "    print(f\"Eval set: {len(eval_set)} methods\")\n",
        "    print(f\"Test set: {len(test_set)} methods\")\n",
        "\n",
        "    # Model Training & Evaluation on Student's Data\n",
        "    # Experiment with n = 3, n = 5, and n = 9; select best based on perplexity\n",
        "    best_n = None\n",
        "    best_perplexity = float(\"inf\")\n",
        "    results = {}\n",
        "\n",
        "    for n in [3, 5, 9]:\n",
        "        print(f\"Evaluating {n}-gram model on training data\")\n",
        "        ngram_counts, context_counts = build_ngram_model(train_set, n)\n",
        "        probabilities = compute_probabilities(ngram_counts, context_counts)\n",
        "\n",
        "        pp = perplexity(eval_set, probabilities, n)\n",
        "        results[n] = pp\n",
        "        print(f\"{n}-gram model Perplexity on Eval set: {pp}\")\n",
        "\n",
        "        if pp < best_perplexity:\n",
        "            best_perplexity = pp\n",
        "            best_n = n\n",
        "\n",
        "    print(f\"Selected best N: {best_n} (Perplexity: {best_perplexity})\")\n",
        "    # According to our report, n = 3 was selected (with perplexity ~91.297)\n",
        "\n",
        "    # Train best model on full training set\n",
        "    best_ngram_counts, best_context_counts = build_ngram_model(train_set, best_n)\n",
        "    best_probabilities = compute_probabilities(best_ngram_counts, best_context_counts)\n",
        "\n",
        "    # Compute perplexity on the full test set for student model\n",
        "    test_pp = perplexity(test_set, best_probabilities, best_n)\n",
        "    print(f\"Test set Perplexity for {best_n}-gram model: {test_pp}\")  # Replace YYY.00000 with actual value\n",
        "\n",
        "    # Generate JSON output for student model (first 100 predictions)\n",
        "    generate_results_json(test_set, best_probabilities, best_n, \"results_student_model.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQrr4pxa6WRb"
      },
      "outputs": [],
      "source": [
        "def teacher_train():\n",
        "    # Load dataset from cleaned corpus (e.g., 7561 methods)\n",
        "    with open(\"/content/drive/Shareddrives/CSCI_420/data/new.txt\", \"r\") as f:\n",
        "        corpus = [line.strip() for line in f]\n",
        "\n",
        "    # Training, Evaluation, and Testing on the Instructor-Provided Corpus\n",
        "    with open(\"/content/drive/Shareddrives/CSCI_420/data/training.txt\", \"r\") as f:\n",
        "        instructor_corpus = [line.strip() for line in f]\n",
        "\n",
        "    print(f\"Total methods in instructor's corpus: {len(instructor_corpus)}\")\n",
        "\n",
        "    # Vocabulary Generation (using training + eval + test sets)\n",
        "    vocab = generate_vocabulary(instructor_corpus)\n",
        "    # print(vocab)\n",
        "    print(f\"Vocabulary size: {len(vocab)}\")  # ZZZ number of code tokens\n",
        "\n",
        "    random.shuffle(instructor_corpus)\n",
        "    test_set = corpus[int(0.9 * len(corpus)):]\n",
        "    instructor_train_set = instructor_corpus[:int(0.8 * len(instructor_corpus))]\n",
        "    instructor_eval_set = instructor_corpus[int(0.8 * len(instructor_corpus)):int(0.9 * len(instructor_corpus))]\n",
        "\n",
        "    best_instructor_n = None\n",
        "    best_instructor_perplexity = float(\"inf\")\n",
        "\n",
        "    for n in [3, 5, 9]:\n",
        "        print(f\"Evaluating {n}-gram model on instructor's training data\")\n",
        "        ngram_counts, context_counts = build_ngram_model(instructor_train_set, n)\n",
        "        probabilities = compute_probabilities(ngram_counts, context_counts)\n",
        "\n",
        "        pp = perplexity(instructor_eval_set, probabilities, n)\n",
        "        print(f\"{n}-gram model Perplexity on Instructor's Eval set: {pp}\")\n",
        "        if pp < best_instructor_perplexity:\n",
        "            best_instructor_perplexity = pp\n",
        "            best_instructor_n = n\n",
        "\n",
        "    print(f\"Best N for instructor model: {best_instructor_n} (Perplexity: {best_instructor_perplexity})\")\n",
        "    # According to our report, n = 3 was selected for the instructor corpus as well\n",
        "\n",
        "    # Train best instructor model\n",
        "    best_instructor_ngram_counts, best_instructor_context_counts = build_ngram_model(instructor_train_set, best_instructor_n)\n",
        "    best_instructor_probabilities = compute_probabilities(best_instructor_ngram_counts, best_instructor_context_counts)\n",
        "\n",
        "    # Compute perplexity on test set for instructor model\n",
        "    instructor_test_pp = perplexity(test_set, best_instructor_probabilities, best_instructor_n)\n",
        "    print(f\"Test set Perplexity for instructor's {best_instructor_n}-gram model: {instructor_test_pp}\")\n",
        "\n",
        "    # Generate JSON output for instructor model (first 100 predictions)\n",
        "    generate_results_json(test_set, best_instructor_probabilities, best_instructor_n, \"results_teacher_model.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI-syaud9Ab6",
        "outputId": "98db6ad0-65fc-4404-e34c-d48b1028b236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total methods in corpus: 7561\n",
            "Vocabulary size: 22520\n",
            "Train set: 6048 methods\n",
            "Eval set: 756 methods\n",
            "Test set: 757 methods\n",
            "Evaluating 3-gram model on training data\n",
            "Number of N-grams: 128631\n",
            "3-gram model Perplexity on Eval set: 176.26594956518113\n",
            "Evaluating 5-gram model on training data\n",
            "Number of N-grams: 212963\n",
            "5-gram model Perplexity on Eval set: 2339.5797926927153\n",
            "Evaluating 9-gram model on training data\n",
            "Number of N-grams: 266052\n",
            "9-gram model Perplexity on Eval set: 50787.560330229826\n",
            "Selected best N: 3 (Perplexity: 176.26594956518113)\n",
            "Number of N-grams: 128631\n",
            "Test set Perplexity for 3-gram model: 150.12739161856175\n",
            "Saved predictions to results_student_model.json\n",
            "First prediction for method 0: ('void', 0.304)\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    student_train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibYolWMa9HBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce03a77-b2e2-4fcb-c220-363e71e7d300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total methods in instructor's corpus: 100000\n",
            "Vocabulary size: 86278\n",
            "Evaluating 3-gram model on instructor's training data\n",
            "Number of N-grams: 353141\n",
            "3-gram model Perplexity on Instructor's Eval set: 25.963798585313025\n",
            "Evaluating 5-gram model on instructor's training data\n",
            "Number of N-grams: 647386\n",
            "5-gram model Perplexity on Instructor's Eval set: 84.54068447123491\n",
            "Evaluating 9-gram model on instructor's training data\n",
            "Number of N-grams: 931379\n",
            "9-gram model Perplexity on Instructor's Eval set: 1214.547920390928\n",
            "Best N for instructor model: 3 (Perplexity: 25.963798585313025)\n",
            "Number of N-grams: 353141\n",
            "Test set Perplexity for instructor's 3-gram model: 5757.829331365027\n",
            "Saved predictions to results_teacher_model.json\n",
            "First prediction for method 0: ('accept', 0.222)\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    teacher_train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcW6VQVITxAy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b51144c4-a6f1-45e5-bb6d-32d90d381ab1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_24e5e098-ba74-44d6-a0fc-5cd2ad2099aa\", \"results_student_model.json\", 55791)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_968d612a-21ca-4a43-b10b-a68ba1138ed5\", \"results_teacher_model.json\", 54244)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('results_student_model.json')\n",
        "files.download('results_teacher_model.json')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}