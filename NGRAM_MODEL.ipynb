{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theantigone/ngram-java-ai/blob/main/NGRAM_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oESE95McS5mN",
        "outputId": "9d659340-4173-48b1-d1f9-1ef0170f0fc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0KnHqXZm2LVZ"
      },
      "outputs": [],
      "source": [
        "from pygments.lexers.jvm import JavaLexer\n",
        "from pygments.token import Token\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "\n",
        "def tokenize_java_code(code):\n",
        "    \"\"\"Tokenizes Java code using Pygments.\"\"\"\n",
        "    lexer = JavaLexer()\n",
        "    return [t[1] for t in lexer.get_tokens(code) if t[0] not in Token.Text]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngram_model(corpus, n):\n",
        "    \"\"\"Builds an N-gram model from tokenized Java methods.\"\"\"\n",
        "    ngram_counts = defaultdict(int)\n",
        "    context_counts = defaultdict(int)\n",
        "\n",
        "    for method in corpus:\n",
        "        tokens = tokenize_java_code(method)\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngram = tuple(tokens[i:i+n])\n",
        "            context = tuple(tokens[i:i+n-1])\n",
        "            ngram_counts[ngram] += 1\n",
        "            context_counts[context] += 1\n",
        "\n",
        "    return ngram_counts, context_counts\n"
      ],
      "metadata": {
        "id": "qd6jqwVI7ssu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_probabilities(ngram_counts, context_counts):\n",
        "    \"\"\"Computes conditional probabilities for N-grams.\"\"\"\n",
        "    probabilities = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        context = ngram[:-1]\n",
        "        probabilities[ngram] = count / context_counts[context]\n",
        "    return probabilities\n"
      ],
      "metadata": {
        "id": "rEUJriH57v_F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(test_corpus, probabilities, n):\n",
        "    \"\"\"Computes perplexity for the N-gram model on a test set.\"\"\"\n",
        "    log_prob_sum = 0\n",
        "    N = 0\n",
        "\n",
        "    for method in test_corpus:\n",
        "        tokens = tokenize_java_code(method)\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngram = tuple(tokens[i:i+n])\n",
        "            prob = probabilities.get(ngram, 1e-6)  # Smoothing for unseen cases\n",
        "            log_prob_sum += math.log2(prob)\n",
        "            N += 1\n",
        "\n",
        "    return 2 ** (-log_prob_sum / N)\n"
      ],
      "metadata": {
        "id": "VTFQ5cCG7xaO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_prediction_x(probabilities, context, n):\n",
        "    \"\"\"Predicts the next token iteratively given a starting context.\"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for _ in range(10):  # Limit predictions to avoid infinite loops\n",
        "        candidates = {ngram[-1]: prob for ngram, prob in probabilities.items() if ngram[:-1] == context}\n",
        "\n",
        "        if not candidates:\n",
        "            break  # Stop if no valid prediction\n",
        "\n",
        "        next_token = max(candidates, key=candidates.get)  # Most probable token\n",
        "        predictions.append((next_token, round(candidates[next_token], 3)))\n",
        "        context = context[1:] + (next_token,)  # Update context\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "Vos0hWE07y1Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_results_json(test_corpus, probabilities, n, filename):\n",
        "    \"\"\"Generates a JSON file with token predictions and probabilities.\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for i, method in enumerate(test_corpus[:100]):  # Limit to 100 examples\n",
        "        tokens = tokenize_java_code(method)[:n-1]\n",
        "        context = tuple(tokens)\n",
        "        results[str(i)] = iterative_prediction_x(probabilities, context, n)\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    print(f\"Saved predictions to {filename}\")\n"
      ],
      "metadata": {
        "id": "whZOvJmK70X1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load dataset\n",
        "    with open(\"/content/drive/Shareddrives/CSCI_420/data/new.txt\", \"r\") as f:\n",
        "        corpus = [line.strip() for line in f]\n",
        "\n",
        "    # Shuffle and split dataset\n",
        "    random.shuffle(corpus)\n",
        "    train_set = corpus[:int(0.8 * len(corpus))]\n",
        "    eval_set = corpus[int(0.8 * len(corpus)):int(0.9 * len(corpus))]\n",
        "    test_set = corpus[int(0.9 * len(corpus)):]\n",
        "\n",
        "    # Train and evaluate for different N values\n",
        "    best_n = None\n",
        "    best_perplexity = float(\"inf\")\n",
        "    results = {}\n",
        "\n",
        "    for n in [3, 5, 7]:\n",
        "        ngram_counts, context_counts = build_ngram_model(train_set, n)\n",
        "        probabilities = compute_probabilities(ngram_counts, context_counts)\n",
        "\n",
        "        pp = perplexity(eval_set, probabilities, n)\n",
        "        results[n] = pp\n",
        "\n",
        "        if pp < best_perplexity:\n",
        "            best_perplexity = pp\n",
        "            best_n = n\n",
        "\n",
        "    print(f\"Best N: {best_n}, Perplexity: {best_perplexity}\")\n",
        "\n",
        "    # Train best model\n",
        "    best_ngram_counts, best_context_counts = build_ngram_model(train_set, best_n)\n",
        "    best_probabilities = compute_probabilities(best_ngram_counts, best_context_counts)\n",
        "\n",
        "    # Generate JSON output for student model\n",
        "    generate_results_json(test_set, best_probabilities, best_n, \"results_student_model.json\")\n",
        "\n",
        "    # Train model on instructor's dataset\n",
        "    with open(\"/content/drive/Shareddrives/CSCI_420/data/training.txt\", \"r\") as f:\n",
        "        instructor_corpus = [line.strip() for line in f]\n",
        "\n",
        "    random.shuffle(instructor_corpus)\n",
        "    instructor_train_set = instructor_corpus[:int(0.8 * len(instructor_corpus))]\n",
        "    instructor_eval_set = instructor_corpus[int(0.8 * len(instructor_corpus)):int(0.9 * len(instructor_corpus))]\n",
        "\n",
        "    # Find best N for instructor's dataset\n",
        "    best_instructor_n = None\n",
        "    best_instructor_perplexity = float(\"inf\")\n",
        "\n",
        "    for n in [3, 5, 7]:\n",
        "        ngram_counts, context_counts = build_ngram_model(instructor_train_set, n)\n",
        "        probabilities = compute_probabilities(ngram_counts, context_counts)\n",
        "\n",
        "        pp = perplexity(instructor_eval_set, probabilities, n)\n",
        "        if pp < best_instructor_perplexity:\n",
        "            best_instructor_perplexity = pp\n",
        "            best_instructor_n = n\n",
        "\n",
        "    print(f\"Best N for instructor model: {best_instructor_n}, Perplexity: {best_instructor_perplexity}\")\n",
        "\n",
        "    # Train best instructor model\n",
        "    best_instructor_ngram_counts, best_instructor_context_counts = build_ngram_model(instructor_train_set, best_instructor_n)\n",
        "    best_instructor_probabilities = compute_probabilities(best_instructor_ngram_counts, best_instructor_context_counts)\n",
        "\n",
        "    # Generate JSON output for instructor model\n",
        "    generate_results_json(test_set, best_instructor_probabilities, best_instructor_n, \"results_teacher_model.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAjzCDKQ71Vh",
        "outputId": "f1ef2a75-df7a-457c-dd81-3da2250b8759"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best N: 3, Perplexity: 34.38444856185524\n",
            "Saved predictions to results_student_model.json\n",
            "Best N for instructor model: 3, Perplexity: 26.398874090998675\n",
            "Saved predictions to results_teacher_model.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on student dataset only\n",
        "def main():\n",
        "    # Load dataset\n",
        "    with open(\"/content/drive/Shareddrives/CSCI_420/data/new.txt\", \"r\") as f:\n",
        "        corpus = [line.strip() for line in f]\n",
        "\n",
        "    # Shuffle and split dataset\n",
        "    random.shuffle(corpus)\n",
        "    train_set = corpus[:int(0.8 * len(corpus))]\n",
        "    eval_set = corpus[int(0.8 * len(corpus)):int(0.9 * len(corpus))]\n",
        "    test_set = corpus[int(0.9 * len(corpus)):]\n",
        "\n",
        "    # Train and evaluate for different N values\n",
        "    best_n = None\n",
        "    best_perplexity = float(\"inf\")\n",
        "    results = {}\n",
        "\n",
        "    for n in [3, 5, 7]:\n",
        "        ngram_counts, context_counts = build_ngram_model(train_set, n)\n",
        "        probabilities = compute_probabilities(ngram_counts, context_counts)\n",
        "\n",
        "        pp = perplexity(eval_set, probabilities, n)\n",
        "        results[n] = pp\n",
        "\n",
        "        if pp < best_perplexity:\n",
        "            best_perplexity = pp\n",
        "            best_n = n\n",
        "\n",
        "    print(f\"Best N: {best_n}, Perplexity: {best_perplexity}\")\n",
        "\n",
        "    # Train best model\n",
        "    best_ngram_counts, best_context_counts = build_ngram_model(train_set, best_n)\n",
        "    best_probabilities = compute_probabilities(best_ngram_counts, best_context_counts)\n",
        "\n",
        "    # Generate JSON output for student model\n",
        "    generate_results_json(test_set, best_probabilities, best_n, \"results_student_model.json\")\n",
        "\n",
        "    # # Train model on instructor's dataset\n",
        "    # with open(\"/content/drive/Shareddrives/CSCI_420/data/training.txt\", \"r\") as f:\n",
        "    #     instructor_corpus = [line.strip() for line in f]\n",
        "\n",
        "    # random.shuffle(instructor_corpus)\n",
        "    # instructor_train_set = instructor_corpus[:int(0.8 * len(instructor_corpus))]\n",
        "    # instructor_eval_set = instructor_corpus[int(0.8 * len(instructor_corpus)):int(0.9 * len(instructor_corpus))]\n",
        "\n",
        "    # # Find best N for instructor's dataset\n",
        "    # best_instructor_n = None\n",
        "    # best_instructor_perplexity = float(\"inf\")\n",
        "\n",
        "    # for n in [3, 5, 7]:\n",
        "    #     ngram_counts, context_counts = build_ngram_model(instructor_train_set, n)\n",
        "    #     probabilities = compute_probabilities(ngram_counts, context_counts)\n",
        "\n",
        "    #     pp = perplexity(instructor_eval_set, probabilities, n)\n",
        "    #     if pp < best_instructor_perplexity:\n",
        "    #         best_instructor_perplexity = pp\n",
        "    #         best_instructor_n = n\n",
        "\n",
        "    # print(f\"Best N for instructor model: {best_instructor_n}, Perplexity: {best_instructor_perplexity}\")\n",
        "\n",
        "    # # Train best instructor model\n",
        "    # best_instructor_ngram_counts, best_instructor_context_counts = build_ngram_model(instructor_train_set, best_instructor_n)\n",
        "    # best_instructor_probabilities = compute_probabilities(best_instructor_ngram_counts, best_instructor_context_counts)\n",
        "\n",
        "    # # Generate JSON output for instructor model\n",
        "    # generate_results_json(test_set, best_instructor_probabilities, best_instructor_n, \"results_teacher_model.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8zSOAveZDFd",
        "outputId": "beb75f38-85fb-4bd5-ccfc-61b81fa85df2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best N: 3, Perplexity: 84.12493880450842\n",
            "Saved predictions to results_student_model.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('results_student_model.json')\n",
        "files.download('results_teacher_model.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZcW6VQVITxAy",
        "outputId": "5f54aedf-2e3f-4d1e-9d97-9517eac55419"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_139a93be-4371-4c24-89ad-0333e6945aed\", \"results_student_model.json\", 31205)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0Ibnx-fUAGy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}